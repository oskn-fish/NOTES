# 各パーセプトロンの入力は1つ．出力も1つ．
前の層のパーセプトロンが発火していて，それらを重み付けして入力とする．だから，
$$
\bm{Wx+b}
$$
というのが入力になる．これを $\bm{u}$ とする．それによる発火 $f(\bm{u})$ を $\bm{z}$ とする．

# 活性化関数は，普通中間層とは異なるものを用いる

# loss functionは問題によって使い分ける
## 回帰
二乗誤差を使う
$$
E(w) = \sum^n_{i=1}\frac{1}{2}||\bm{d}_n-y(\bm{x}_n;\bm{w})||^2
$$

## 二値分類
データ$\bm{x}$を与えたときの事後確率 $p(\bm{d}|\bm{x})$ をNNでモデル化する．
出力層のパーセプトロンは一つにして，その出力を
$$
y = \frac{1}{1+\exp(-u)}
$$
とすればよい．

**最尤推定を行えば良いので，**
$$\begin{align}
p(\bm{d}|\bm{x}) &= p(\bm{d}=1|\bm{x})^dp(\bm{d}=0|\bm{x})^{1-d}\\
&= p(\bm{d}=1|\bm{x})^d(1-p(\bm{d}=1|\bm{x}))^{1-d}
\end{align}
$$
から，
$$\begin{align}
L(w) &= -\prod^n_{i=1}p(d_i|x_i)=-\prod^n_{i=1}p(\bm{d}=1|\bm{x})^d(1-p(\bm{d}=1|\bm{x}))^{1-d}\\
&= -\prod^n_{i=1}y(\bm{x}_n|\bm{w})^d(1-y(\bm{x}_n|\bm{w}))^{1-d}
\end{align}
$$

## 他クラス分類
出力層は分類クラス分のパーセプトロンを用意する．そのパーセプトロンのsoftmaxを取って，各クラスの確率とする

他クラス分類でも同様に．尤度最大化を行う．

$\bm{d}_n$がone-hotベクトルになっているとき，一つのデータに対する尤度は
$$
p(\bm{d}|\bm{x}) = \prod^c_{i=1}p(\bm{d}=i|\bm{x})^{d_i}
$$
となるため，尤度は
$$
\prod^n_{i=1}p(\bm{d}_i|\bm{x_i})=\prod^n_{i=1}\prod^c_{j=1}p(\bm{d}_i=j|\bm{x_i})^{d_i^j}
$$
対数を取って，負を取ると
$$
-\sum^n_{i=1}\sum^c_{j=1}d_i^j\log(p(d_i=j|x_i))
$$
**これを交差エントロピーという．**

## 交差エントロピーは他クラス分類において尤度を最大化する．

## softmax関数の冗長性への対策
softmax関数を取るとき，出力層は，定数のシフトに対して同じsoftmaxの値を取る．このような冗長性には，出力に制約式を一つ加えれば自由度を減らすことができて解決できる．

例えば，ある出力を$0$にするなど．
